{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from pdfminer.high_level import extract_text\n",
    "from io import BytesIO\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Inisialisasi Direktori\n",
    "def initialize_directories():\n",
    "    \"\"\"Membuat struktur direktori yang diperlukan\"\"\"\n",
    "    Path('data/raw').mkdir(parents=True, exist_ok=True)\n",
    "    Path('logs').mkdir(parents=True, exist_ok=True)\n",
    "    Path('figures').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Buat file log jika belum ada\n",
    "    if not os.path.exists('logs/cleaning.log'):\n",
    "        with open('logs/cleaning.log', 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Log file created at {datetime.now()}\\n\")\n",
    "\n",
    "# 2. Fungsi Bantuan\n",
    "def get_detail(soup, keyword):\n",
    "    \"\"\"Mengekstrak detail dari tabel metadata\"\"\"\n",
    "    try:\n",
    "        return soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text).find_next().get_text().strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Membersihkan teks dari header/footer dan normalisasi\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Hapus header/footer spesifik\n",
    "    patterns = [\n",
    "        r\"M a h ka m a h A g u n g R e p u blik In d o n esia[\\s\\S]*?Kepaniteraan Mahkamah Agung RI\",\n",
    "        r\"Disclaimer[\\s\\S]*?ext\\.318\\)\",\n",
    "        r\"Halaman \\d+ dari \\d+\",\n",
    "        r\"Page \\d+ of \\d+\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalisasi teks\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def download_pdf(url):\n",
    "    \"\"\"Mengunduh dan mengekstrak teks dari PDF\"\"\"\n",
    "    try:\n",
    "        # Unduh PDF\n",
    "        pdf_file = urllib.request.urlopen(url)\n",
    "        file_content = pdf_file.read()\n",
    "        \n",
    "        # Ekstrak teks\n",
    "        text = extract_text(BytesIO(file_content))\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 3. Fungsi Ekstraksi Dokumen Utama\n",
    "def extract_document_data(url, max_retries=3):\n",
    "    \"\"\"Mengekstrak data dari halaman putusan\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Ekstrak metadata\n",
    "            metadata = {\n",
    "                'judul': soup.find('h2').get_text().strip() if soup.find('h2') else \"\",\n",
    "                'nomor': get_detail(soup, \"Nomor\"),\n",
    "                'tanggal': get_detail(soup, \"Tanggal Register\"),\n",
    "                'jenis_perkara': get_detail(soup, \"Jenis Lembaga Peradilan\"),\n",
    "                'pasal': get_detail(soup, \"Kaidah\"),\n",
    "                'amar': get_detail(soup, \"Amar\"),\n",
    "                'link': url\n",
    "            }\n",
    "            \n",
    "            # Ekstrak teks PDF\n",
    "            pdf_link = soup.find('a', href=re.compile(r'/pdf/'))\n",
    "            if pdf_link:\n",
    "                pdf_url = urljoin(url, pdf_link['href'])\n",
    "                pdf_text = download_pdf(pdf_url)\n",
    "                metadata['text'] = clean_text(pdf_text) if pdf_text else \"\"\n",
    "            else:\n",
    "                metadata['text'] = \"\"\n",
    "            \n",
    "            return metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {url}: {e}\")\n",
    "            time.sleep(5)  # Tunggu sebelum retry\n",
    "    \n",
    "    print(f\"Max retries reached for {url}\")\n",
    "    return None\n",
    "\n",
    "# 4. Fungsi Penyimpanan\n",
    "def save_document(data):\n",
    "    \"\"\"Menyimpan dokumen dan mencatat log\"\"\"\n",
    "    try:\n",
    "        # Hitung ID dokumen\n",
    "        existing_files = [f for f in os.listdir('data/raw') if f.startswith('case_')]\n",
    "        doc_id = len(existing_files) + 1\n",
    "        \n",
    "        # Simpan teks putusan\n",
    "        with open(f'data/raw/case_{doc_id:03d}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(data.get('text', ''))\n",
    "        \n",
    "        # Catat log\n",
    "        with open('logs/cleaning.log', 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"{datetime.now()} - Processed document {doc_id}: {data.get('judul', '')}\\n\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving document: {e}\")\n",
    "        return False\n",
    "\n",
    "# 5. Fungsi Scraping Utama\n",
    "def scrape_ma_putusan(keyword=\"narkotika\", max_documents=30, delay=1):\n",
    "    \"\"\"Fungsi utama untuk scraping putusan MA\"\"\"\n",
    "    # Inisialisasi\n",
    "    initialize_directories()\n",
    "    base_url = \"https://putusan3.mahkamahagung.go.id\"\n",
    "    search_url = f\"{base_url}/search.html\"\n",
    "    documents = []\n",
    "    page = 1\n",
    "    \n",
    "    print(f\"Memulai scraping untuk keyword: '{keyword}'\")\n",
    "    \n",
    "    while len(documents) < max_documents:\n",
    "        try:\n",
    "            # Buat URL pencarian\n",
    "            url = f\"{search_url}?q={urllib.parse.quote(keyword)}&page={page}&obf=TANGGAL_PUTUS&obm=desc\"\n",
    "            \n",
    "            # Request halaman\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Temukan semua link putusan\n",
    "            links = soup.find_all('a', href=re.compile(r'/direktori/putusan/'))\n",
    "            \n",
    "            if not links:\n",
    "                print(\"Tidak menemukan hasil lagi.\")\n",
    "                break\n",
    "                \n",
    "            print(f\"Memproses halaman {page} - ditemukan {len(links)} putusan\")\n",
    "            \n",
    "            # Proses setiap putusan\n",
    "            for link in links:\n",
    "                if len(documents) >= max_documents:\n",
    "                    break\n",
    "                    \n",
    "                doc_path = link['href']\n",
    "                doc_url = urljoin(base_url, doc_path)\n",
    "                \n",
    "                print(f\"Memproses: {doc_url}\")\n",
    "                \n",
    "                # Ekstrak data\n",
    "                doc_data = extract_document_data(doc_url)\n",
    "                if doc_data and doc_data.get('text'):\n",
    "                    if save_document(doc_data):\n",
    "                        documents.append(doc_data)\n",
    "                        print(f\"Berhasil menyimpan dokumen {len(documents)}\")\n",
    "                    else:\n",
    "                        print(\"Gagal menyimpan dokumen\")\n",
    "                \n",
    "                # Jeda antara request\n",
    "                time.sleep(delay)\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error request: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Simpan metadata\n",
    "    if documents:\n",
    "        df = pd.DataFrame(documents)\n",
    "        df.to_csv('data/metadata_raw.csv', index=False)\n",
    "        print(f\"Berhasil menyimpan {len(df)} dokumen ke data/metadata_raw.csv\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Tidak ada dokumen yang berhasil diambil\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# 6. Eksekusi\n",
    "if __name__ == \"__main__\":\n",
    "    # Contoh penggunaan\n",
    "    keyword = \"narkotika\"  # Ganti dengan keyword yang diinginkan\n",
    "    max_docs = 30          # Jumlah dokumen yang ingin diambil\n",
    "    \n",
    "    start_time = time.time()\n",
    "    df = scrape_ma_putusan(keyword=keyword, max_documents=max_docs)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nSelesai dalam {elapsed:.2f} detik\")\n",
    "    print(f\"Total dokumen yang berhasil diambil: {len(df)}\")\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(\"\\nContoh data:\")\n",
    "        print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
