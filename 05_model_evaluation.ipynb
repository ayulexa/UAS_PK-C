{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION.ipynb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import json\n",
    "import seaborn as sns # Import seaborn\n",
    "\n",
    "# 1. Evaluasi Retrieval\n",
    "def load_retrieval_metrics():\n",
    "    metrics_df = pd.read_csv('data/eval/retrieval_metrics.csv', index_col=0)\n",
    "\n",
    "    # Plot perbandingan metrik\n",
    "    metrics_df.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title('Perbandingan Metrik Retrieval')\n",
    "    plt.ylabel('Skor')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.savefig('figures/retrieval_metrics.png')\n",
    "    plt.show()\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "# 2. Evaluasi Prediksi (Solution Reuse)\n",
    "def evaluate_predictions():\n",
    "    # Load ground truth (dalam implementasi nyata, ini harus disiapkan)\n",
    "    # Contoh sederhana:\n",
    "    ground_truth = {\n",
    "        \"pelaku ditangkap dengan barang bukti sabu 1 gram\": \"Pidana Penjara\",\n",
    "        \"terdakwa mengedarkan ganja seberat 500 gram\": \"Pidana Penjara\",\n",
    "        \"pemakai narkotika jenis ekstasi\": \"Rehabilitasi\"\n",
    "    }\n",
    "\n",
    "    # Load predictions\n",
    "    pred_df = pd.read_csv('data/results/predictions.csv')\n",
    "\n",
    "    # Hitung metrik\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # Filter predictions to only include queries present in ground_truth\n",
    "    pred_df_filtered = pred_df[pred_df['query'].isin(ground_truth.keys())].copy()\n",
    "\n",
    "    for _, row in pred_df_filtered.iterrows():\n",
    "        y_true.append(ground_truth[row['query']])\n",
    "        y_pred.append(row['predicted_solution'])\n",
    "\n",
    "    # Handle cases where y_true or y_pred are empty to avoid errors in metrics calculation\n",
    "    if not y_true:\n",
    "        print(\"No predictions match ground truth queries for evaluation.\")\n",
    "        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        cm = [] # Empty confusion matrix\n",
    "    else:\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0), # Add zero_division\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),     # Add zero_division\n",
    "            'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0)             # Add zero_division\n",
    "        }\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=list(set(y_true + y_pred))) # Ensure all labels are included\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(set(y_true + y_pred)), yticklabels=list(set(y_true + y_pred)))\n",
    "        plt.title('Confusion Matrix Prediksi')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig('figures/confusion_matrix.png')\n",
    "        plt.show()\n",
    "\n",
    "    # Simpan metrik\n",
    "    pd.DataFrame.from_dict(metrics, orient='index', columns=['Value']).to_csv('data/eval/prediction_metrics.csv')\n",
    "\n",
    "\n",
    "    return metrics, ground_truth # Return ground_truth as well\n",
    "\n",
    "\n",
    "# 3. Analisis kesalahan\n",
    "def error_analysis(ground_truth):\n",
    "    \"\"\"Contoh analisis kasus yang salah prediksi\"\"\"\n",
    "    print(\"\\nContoh Kasus yang Salah Prediksi:\")\n",
    "\n",
    "    # Load data untuk contoh\n",
    "    df = pd.read_csv('data/results/predictions.csv')\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row['query'] in ground_truth and row['predicted_solution'] != ground_truth[row['query']]:\n",
    "            print(f\"\\nQuery: {row['query']}\")\n",
    "            print(f\"Prediksi: {row['predicted_solution']}\")\n",
    "            print(f\"Sebenarnya: {ground_truth[row['query']]}\")\n",
    "        # else: # Optional: print correctly predicted cases\n",
    "        #     if row['query'] in ground_truth and row['predicted_solution'] == ground_truth[row['query']]:\n",
    "        #          print(f\"\\nCorrectly Predicted: {row['query']}\")\n",
    "        #          print(f\"Prediction/Actual: {row['predicted_solution']}\")\n",
    "\n",
    "\n",
    "# Jalankan evaluasi\n",
    "retrieval_metrics = load_retrieval_metrics()\n",
    "prediction_metrics, ground_truth = evaluate_predictions() # Get ground_truth here\n",
    "\n",
    "print(\"Metrik Retrieval:\")\n",
    "print(retrieval_metrics)\n",
    "\n",
    "print(\"\\nMetrik Prediksi:\")\n",
    "print(prediction_metrics)\n",
    "\n",
    "error_analysis(ground_truth) # Pass ground_truth to error_analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
